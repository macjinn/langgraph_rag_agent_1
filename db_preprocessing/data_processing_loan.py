from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait, Select
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.action_chains import ActionChains
from bs4 import BeautifulSoup
import time
import json
import re

# ê³µí†µ ì •ë¦¬ í•¨ìˆ˜
def clean(text):
    if not isinstance(text, str):
        return ""
    return " ".join(text.replace("\n", " ").replace("\t", " ").replace("\xa0", " ").split())

# ì£¼íƒë‹´ë³´ëŒ€ì¶œ ìƒì„¸ ì •ê·œì‹ íŒ¨í„´
section_patterns = {
    "ë¹„êµê³µì‹œì¼": r"ë¹„êµê³µì‹œì¼\s*:\s*([^\n ]+)",
    "ë‹´ë‹¹ë¶€ì„œ ë° ì—°ë½ì²˜": r"ë‹´ë‹¹ë¶€ì„œ ë° ì—°ë½ì²˜\s*([^\n]+?)ê°€ì…ë°©ë²•",
    "ê°€ì…ë°©ë²•": r"ê°€ì…ë°©ë²•\s*([^\n]+)",
    "ëŒ€ì¶œ ë¶€ëŒ€ë¹„ìš©": r"ëŒ€ì¶œ ë¶€ëŒ€ë¹„ìš©[^\d]*(\d\..*?)ì¤‘ë„ìƒí™˜ ìˆ˜ìˆ˜ë£Œ",
    "ì¤‘ë„ìƒí™˜ ìˆ˜ìˆ˜ë£Œ": r"ì¤‘ë„ìƒí™˜ ìˆ˜ìˆ˜ë£Œ[^\d]*(ì¤‘ë„ìƒí™˜ì›ê¸ˆ.*?)(ì—°ì²´ ì´ììœ¨|ëŒ€ì¶œí•œë„)",
    "ì—°ì²´ ì´ììœ¨": r"ì—°ì²´ ì´ììœ¨\s*([^\n]+)",
    "ëŒ€ì¶œí•œë„": r"ëŒ€ì¶œí•œë„\s*([^\n]+)",
    "PDF": r"-"
}

def parse_detail_text(detail_text):
    detail_dict = {}
    fields = [
        "ë¹„êµê³µì‹œì¼", "ë‹´ë‹¹ë¶€ì„œ ë° ì—°ë½ì²˜", "ê°€ì…ë°©ë²•", 
        "ëŒ€ì¶œ ë¶€ëŒ€ë¹„ìš©", "ì¤‘ë„ìƒí™˜ ìˆ˜ìˆ˜ë£Œ", "ì—°ì²´ ì´ììœ¨", "ëŒ€ì¶œí•œë„"
    ]
    for field in fields:
        pattern = re.search(f"{field}\\s+([^ë¹„êµê³µì‹œì¼ë‹´ë‹¹ë¶€ì„œë°ì—°ë½ì²˜ê°€ì…ë°©ë²•ëŒ€ì¶œë¶€ëŒ€ë¹„ìš©ì¤‘ë„ìƒí™˜ìˆ˜ìˆ˜ë£Œì—°ì²´ì´ììœ¨ëŒ€ì¶œí•œë„]+)", detail_text)
        if pattern:
            detail_dict[field] = clean(pattern.group(1))
    return detail_dict


def extract_selected_value(raw_text):
    if not raw_text:
        return ""
    return clean(raw_text.split()[-1])


def extract_sections(detail_text):
    detail_text = re.sub(r"ê¸ˆìœµê±°ë˜\s*ê³„ì‚°ê¸°\s*ì†Œë¹„ìë³´í˜¸ì •ë³´\s*ê²½ì˜ì •ë³´", "", detail_text)
    sections = {}

    score_levels = [
        "900ì  ì´ˆê³¼", "801~900ì ", "701~800ì ", "601~700ì ",
        "501~600ì ", "401~500ì ", "301~400ì ", "300ì  ì´í•˜", "í‰ê· ê¸ˆë¦¬"
    ]

    def parse_rate_block(label, block_text):
        values = block_text.strip().split()
        return {f"{label}_{level}": values[i] for i, level in enumerate(score_levels) if i < len(values)}

    pattern_rate_block = re.compile(r"(ê¸°ì¤€ê¸ˆë¦¬|ê°€ì‚°ê¸ˆë¦¬|ê°€[Â·\.]?ê°ì¡°ì •ê¸ˆë¦¬)\s+((?:\d+\.\d+%?\s+)+)")
    for match in pattern_rate_block.finditer(detail_text):
        label = match.group(1).replace("ê°€.ê°", "ê°€Â·ê°").replace("ê°€Â·ê°", "ê°€Â·ê°ì¡°ì •ê¸ˆë¦¬")
        rate_values = match.group(2)
        sections.update(parse_rate_block(label, rate_values))

    for key, pattern in section_patterns.items():
        match = re.search(pattern, detail_text)
        if match:
            sections[key] = match.group(1).strip()

    match = re.search(r"ê³µì‹œëŒ€ìƒìƒí’ˆ\s*(\(.*?\))", detail_text)
    if match:
        sections["ê³µì‹œëŒ€ìƒìƒí’ˆ"] = clean(match.group(1))

    return sections

# ê°œì¸ì‹ ìš©ëŒ€ì¶œ ìƒì„¸ì •ë³´ í›„ì²˜ë¦¬
def parse_personal_detail(detail_text):
    sections = {}
    detail_text = clean(detail_text)

    score_levels = [
        "900ì  ì´ˆê³¼", "801~900ì ", "701~800ì ", "601~700ì ",
        "501~600ì ", "401~500ì ", "301~400ì ", "300ì  ì´í•˜", "í‰ê· ê¸ˆë¦¬"
    ]

    def parse_rate_block(label, block_text):
        values = block_text.strip().split()
        return {f"{label}_{level}": values[i] for i, level in enumerate(score_levels) if i < len(values)}

    pattern_rate_block = re.compile(r"(ê¸°ì¤€ê¸ˆë¦¬|ê°€ì‚°ê¸ˆë¦¬|ê°€[Â·\.]?ê°ì¡°ì •ê¸ˆë¦¬)\s+((?:\d+\.\d+%?\s+)+)")
    for match in pattern_rate_block.finditer(detail_text):
        label = match.group(1).replace("ê°€.ê°", "ê°€Â·ê°").replace("ê°€Â·ê°", "ê°€Â·ê°ì¡°ì •ê¸ˆë¦¬")
        rate_values = match.group(2)
        sections.update(parse_rate_block(label, rate_values))

    match = re.search(r"ë¹„êµê³µì‹œì¼\s*[:ï¼š]?\s*(\d{4}-\d{2}-\d{2})", detail_text)
    if match:
        sections["ë¹„êµê³µì‹œì¼"] = match.group(1)

    match = re.search(r"ë‹´ë‹¹ë¶€ì„œ ë° ì—°ë½ì²˜\s*(.*?)ê°€ì…ë°©ë²•", detail_text)
    if match:
        sections["ë‹´ë‹¹ë¶€ì„œ ë° ì—°ë½ì²˜"] = clean(match.group(1))

    match = re.search(r"ê°€ì…ë°©ë²•\s*([^\n]+)", detail_text)
    if match:
        sections["ê°€ì…ë°©ë²•"] = clean(match.group(1))

    match = re.search(r"ê³µì‹œëŒ€ìƒìƒí’ˆ\s*(\(.*?\))", detail_text)
    if match:
        sections["ê³µì‹œëŒ€ìƒìƒí’ˆ"] = clean(match.group(1))

    return sections

driver = webdriver.Chrome()
wait = WebDriverWait(driver, 15)
driver.get("https://finlife.fss.or.kr/finlife/ldng/houseMrtg/list.do?menuNo=700007")

wait.until(EC.presence_of_element_located((By.ID, "pageUnit")))
select = Select(driver.find_element(By.ID, "pageUnit"))
select.select_by_visible_text("50")
time.sleep(0.5)

wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, "button.search.ajaxFormSearch"))).click()
time.sleep(0.5)

results = []

for page_num in range(1, 7):
    print(f"ğŸ“„ í˜ì´ì§€ {page_num} ìˆ˜ì§‘ ì¤‘...")

    if page_num > 1:
        try:
            page_links = driver.find_elements(By.CSS_SELECTOR, "ul.pagination li a[data-pageindex]")
            for link in page_links:
                if link.get_attribute("data-pageindex") == str(page_num):
                    driver.execute_script("arguments[0].click();", link)
                    time.sleep(0.1)
                    wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, "tbody tr")))
                    break
        except Exception as e:
            print(f"âŒ í˜ì´ì§€ {page_num} ì´ë™ ì‹¤íŒ¨: {e}")
            break

    rows = driver.find_elements(By.CSS_SELECTOR, "tbody tr")
    for i in range(0, len(rows), 2):
        info = {}
        try:
            tds = rows[i].find_elements(By.TAG_NAME, "td")
            if len(tds) < 15:
                continue

            info = {
                "ê¸ˆìœµíšŒì‚¬": clean(tds[1].text),
                "ìƒí’ˆëª…": clean(tds[2].text),
                "ì£¼íƒì¢…ë¥˜": clean(tds[3].text),
                "ê¸ˆë¦¬ë°©ì‹": extract_selected_value(tds[4].text),
                "ìƒí™˜ë°©ì‹": extract_selected_value(tds[5].text),
                "ìµœì €ê¸ˆë¦¬": clean(tds[6].text),
                "ìµœê³ ê¸ˆë¦¬": clean(tds[7].text),
                "ì „ì›”í‰ê· ê¸ˆë¦¬": clean(tds[8].text),
                "ë¬¸ì˜ì „í™”": clean(tds[13].text)
            }

            detail_link = next(
                a for a in rows[i].find_elements(By.TAG_NAME, "a") if "ìƒì„¸" in a.text
            )
            driver.execute_script("arguments[0].scrollIntoView(true);", detail_link)
            driver.execute_script("arguments[0].click();", detail_link)
            time.sleep(0.05)

            # ìƒì„¸ í´ë¦­ ë° í›„ì²˜ë¦¬
            detail_row = rows[i + 1]
            raw_detail = clean(detail_row.text)
            info["ìƒì„¸ì •ë³´"] = raw_detail
            info.update(parse_detail_text(raw_detail))

                        # ğŸ” í›„ì²˜ë¦¬ ì¶”ê°€
            info.update(extract_sections(raw_detail))

        except Exception as e:
            print(f"âŒ ìƒì„¸ í´ë¦­ ì‹¤íŒ¨ @ row {i}: {e}")
            info["ìƒì„¸ì •ë³´"] = ""

        results.append(info)

    print(f"âœ… ëˆ„ì  ìˆ˜ì§‘: {len(results)}ê°œ")

driver.quit()

documents = []
for idx, item in enumerate(results):
    doc = {
        "id": f"ëŒ€ì¶œ_{idx+1:03d}",
        "bank": item.get("ê¸ˆìœµíšŒì‚¬", ""),
        "product_name": item.get("ìƒí’ˆëª…", ""),
        "type": "ì£¼íƒë‹´ë³´ëŒ€ì¶œ",
        "content": f"{item.get('ê¸ˆìœµíšŒì‚¬', '')}, {item.get('ìƒí’ˆëª…', '')}, "
                   f"{item.get('ìµœì €ê¸ˆë¦¬', '')}~{item.get('ìµœê³ ê¸ˆë¦¬', '')}, "
                   f"{item.get('ëŒ€ì¶œí•œë„', item.get('í•œë„', item.get('ëŒ€ì¶œê¸ˆì•¡', '')))}, "
                   f"{item.get('ëŒ€ì¶œê¸°ê°„', '')}",
        "key_summary": f"{item.get('ê¸ˆìœµíšŒì‚¬', '')} / {item.get('ìƒí’ˆëª…', '')} / "
                       f"{item.get('ìµœì €ê¸ˆë¦¬', '')}~{item.get('ìµœê³ ê¸ˆë¦¬', '')} / "
                       f"{item.get('ëŒ€ì¶œí•œë„', item.get('í•œë„', item.get('ëŒ€ì¶œê¸ˆì•¡', '')))} / "
                       f"{item.get('ëŒ€ì¶œê¸°ê°„', '')}",
        "metadata": item
    }
    documents.append(doc)


############################
#######ê°œì¸ì‹ ìš©ëŒ€ì¶œ##########
############################

driver = webdriver.Chrome()
wait = WebDriverWait(driver, 15)
driver.get("https://finlife.fss.or.kr/finlife/ldng/indvlCrdt/list.do?menuNo=700009")

wait.until(EC.presence_of_element_located((By.ID, "pageUnit")))
select = Select(driver.find_element(By.ID, "pageUnit"))
select.select_by_visible_text("50")
time.sleep(0.5)

wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, "button.search.ajaxFormSearch"))).click()
time.sleep(0.5)

results = []

rows = driver.find_elements(By.CSS_SELECTOR, "tbody tr")
for i in range(0, len(rows), 2):
    info = {}
    try:
        tds = rows[i].find_elements(By.TAG_NAME, "td")
        if len(tds) < 15:
            continue

        info = {
            "ê¸ˆìœµíšŒì‚¬": clean(tds[0].text),
            "ëŒ€ì¶œì¢…ë¥˜": clean(tds[1].text),
            "ê¸ˆë¦¬êµ¬ë¶„": clean(tds[2].text),
            "900ì  ì´ˆê³¼": clean(tds[3].text),
            "801~900ì ": clean(tds[4].text),
            "701~800ì ": clean(tds[5].text),
            "601~700ì ": clean(tds[6].text),
            "501~600ì ": clean(tds[7].text),
            "401~500ì ": clean(tds[8].text),
            "301~400ì ": clean(tds[9].text),
            "300ì  ì´í•˜": clean(tds[10].text),
            "í‰ê· ê¸ˆë¦¬": clean(tds[11].text),
            "CB íšŒì‚¬ëª…": clean(tds[12].text),
            "ë¬¸ì˜ì „í™”": clean(tds[13].text)
        }

        detail_link = next(
            a for a in rows[i].find_elements(By.TAG_NAME, "a") if "ìƒì„¸" in a.text
        )
        driver.execute_script("arguments[0].scrollIntoView(true);", detail_link)
        driver.execute_script("arguments[0].click();", detail_link)
        time.sleep(0.05)

        # ìƒì„¸ í´ë¦­ ë° í›„ì²˜ë¦¬
        detail_row = rows[i + 1]
        raw_detail = clean(detail_row.text)
        info["ìƒì„¸ì •ë³´"] = raw_detail
        info.update(parse_detail_text(raw_detail))

        # ğŸ” í›„ì²˜ë¦¬ ì¶”ê°€
        info.update(parse_personal_detail(raw_detail))

    except Exception as e:
        print(f"âŒ ìƒì„¸ í´ë¦­ ì‹¤íŒ¨ @ row {i}: {e}")
        info["ìƒì„¸ì •ë³´"] = ""

    results.append(info)

print(f"âœ… ëˆ„ì  ìˆ˜ì§‘: {len(results)}ê°œ")

driver.quit()

for idx, item in enumerate(results):
    doc = {
        "id": f"ëŒ€ì¶œ_{idx+299:03d}",
        "bank": item.get("ê¸ˆìœµíšŒì‚¬", ""),
        "product_name": item.get("ëŒ€ì¶œì¢…ë¥˜", ""),
        "type": "ê°œì¸ì‹ ìš©ëŒ€ì¶œ",
        "content": f"{item.get('ê¸ˆìœµíšŒì‚¬', '')}, {item.get('ëŒ€ì¶œì¢…ë¥˜', '')}, "
                   f"{item.get('ê¸ˆë¦¬êµ¬ë¶„', '')}~{item.get('í‰ê· ê¸ˆë¦¬', '')}, ",
        "key_summary": f"{item.get('ê¸ˆìœµíšŒì‚¬', '')} / {item.get('ëŒ€ì¶œì¢…ë¥˜', '')} / "
                       f"{item.get('ê¸ˆë¦¬êµ¬ë¶„', '')}~{item.get('í‰ê· ê¸ˆë¦¬', '')} / ",
        "metadata": item
    }
    documents.append(doc)

with open("loan.json", "w", encoding="utf-8") as f:
    json.dump({"documents": documents}, f, ensure_ascii=False, indent=2)

print("ğŸ“ ì €ì¥ ì™„ë£Œ: ëŒ€ì¶œ_ìƒì„¸í¬í•¨.json")