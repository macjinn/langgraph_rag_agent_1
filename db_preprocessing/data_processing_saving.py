from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait, Select
from selenium.webdriver.support import expected_conditions as EC
import time
import json
import re

# í…ìŠ¤íŠ¸ ì •ë¦¬ í•¨ìˆ˜
def clean(text):
    if not isinstance(text, str):
        return ""
    return " ".join(text.replace("\n", " ").replace("\t", " ").replace("\xa0", " ").split())

# ìƒì„¸ì •ë³´ í•„ë“œ ì¶”ì¶œ
savings_detail_patterns = {
    "ë¹„êµê³µì‹œì¼": r"ë¹„êµê³µì‹œì¼\s*[:ï¼š]?\s*([^\n ]+)",
    "ë‹´ë‹¹ë¶€ì„œ ë° ì—°ë½ì²˜": r"ë‹´ë‹¹ë¶€ì„œ ë° ì—°ë½ì²˜\s*(.*?)ìš°ëŒ€ì¡°ê±´",
    "ìš°ëŒ€ì¡°ê±´": r"ìš°ëŒ€ì¡°ê±´\s*([^\n]+)",
    "ê°€ì…ëŒ€ìƒ": r"ê°€ì…ëŒ€ìƒ\s*([^\n]+)",
    "ê°€ì…ë°©ë²•": r"ê°€ì…ë°©ë²•\s*([^\n]+)",
    "ë§Œê¸°í›„ ì´ììœ¨": r"ë§Œê¸°í›„ ì´ììœ¨\s*([^\n]+)",
    "ê¸°íƒ€ ìœ ì˜ì‚¬í•­": r"ê¸°íƒ€ ìœ ì˜ì‚¬í•­\s*([^\n]+)"
}

def parse_savings_detail(text):
    metadata = {}
    text = clean(text)

    # ë¨¼ì € "ê°€ì…ëŒ€ìƒ" ë‚´ìš© ì¶”ì¶œ
    join_target_match = re.search(savings_detail_patterns["ê°€ì…ëŒ€ìƒ"], text)
    join_target_text = join_target_match.group(1).strip() if join_target_match else ""

    for key, pattern in savings_detail_patterns.items():
        match = re.search(pattern, text)
        if match:
            value = match.group(1).strip()
            if key == "ìš°ëŒ€ì¡°ê±´":
                # ê°€ì…ëŒ€ìƒ ë‚´ìš©ì´ ìš°ëŒ€ì¡°ê±´ ì•ˆì— ìˆë‹¤ë©´ ì œê±°
                value = value.replace(join_target_text, "").strip(" ,;:/")
                value = value.replace("ê°€ì…ëŒ€ìƒ","")
            metadata[key] = value

    return metadata

# Selenium ìŠ¤í¬ë˜í•‘ ì‹œì‘
url = "https://finlife.fss.or.kr/finlife/svings/fdrmEnty/list.do?menuNo=700003"
driver = webdriver.Chrome()
wait = WebDriverWait(driver, 15)
driver.get(url)

wait.until(EC.presence_of_element_located((By.ID, "pageUnit")))
select = Select(driver.find_element(By.ID, "pageUnit"))
select.select_by_visible_text("50")
time.sleep(1)

wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, "button.search.ajaxFormSearch"))).click()
time.sleep(1.5)

results = []
for page_num in range(1, 10):
    print(f"ğŸ“„ í˜ì´ì§€ {page_num} ìˆ˜ì§‘ ì¤‘...")

    if page_num > 1:
        try:
            page_links = driver.find_elements(By.CSS_SELECTOR, "ul.pagination li a[data-pageindex]")
            for link in page_links:
                if link.get_attribute("data-pageindex") == str(page_num):
                    driver.execute_script("arguments[0].click();", link)
                    time.sleep(2)
                    wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, "tbody tr")))
                    break
        except Exception as e:
            print(f"âŒ í˜ì´ì§€ {page_num} ì´ë™ ì‹¤íŒ¨: {e}")
            break

    rows = driver.find_elements(By.CSS_SELECTOR, "tbody tr")

    for i in range(0, len(rows), 2):
        try:
            tds = rows[i].find_elements(By.TAG_NAME, "td")
            if len(tds) < 10:
                continue

            info = {
                "ê¸ˆìœµíšŒì‚¬": clean(tds[1].text),
                "ìƒí’ˆëª…": clean(tds[2].text),
                "ì ë¦½ë°©ì‹":clean(tds[3].text),
                "ì„¸ì „ì´ììœ¨":clean(tds[4].text),
                "ì„¸í›„ì´ììœ¨":clean(tds[5].text),
                "ì„¸í›„ì´ì(ì˜ˆì‹œ)":clean(tds[6].text),
                "ìµœê³ ìš°ëŒ€ê¸ˆë¦¬":clean(tds[7].text),
                "ê°€ì…ëŒ€ìƒ":clean(tds[8].text),
                "ì´ìê³„ì‚°ë°©ì‹":clean(tds[9].text),
                "ê¸ˆìœµìƒí’ˆë¬¸ì˜":clean(tds[10].text)
            }

            detail_link = next(a for a in rows[i].find_elements(By.TAG_NAME, "a") if "ìƒì„¸" in a.text)
            driver.execute_script("arguments[0].scrollIntoView(true);", detail_link)
            driver.execute_script("arguments[0].click();", detail_link)
            time.sleep(0.05)

            detail_row = rows[i + 1]
            raw_detail = clean(detail_row.text)
            info["ìƒì„¸ì •ë³´"] = raw_detail
            info.update(parse_savings_detail(raw_detail))

            results.append(info)

        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜ @ row {i}: {e}")

print(f"âœ… ì´ ìˆ˜ì§‘: {len(results)}ê±´")
driver.quit()

# JSON ì €ì¥
documents = []
for idx, item in enumerate(results):
    bank = item.get("ê¸ˆìœµíšŒì‚¬", "")
    product = item.get("ìƒí’ˆëª…", "")
    base_rate = item.get("ì„¸ì „ì´ììœ¨", "")
    max_rate = item.get("ìµœê³ ìš°ëŒ€ê¸ˆë¦¬", "")
    interest_type = item.get("ì´ìœ¨êµ¬ë¶„", "")
    pub_date = item.get("ë¹„êµê³µì‹œì¼", item.get("ê¸°ì¤€ì¼", ""))
    join_method = item.get("ê°€ì…ë°©ë²•", "")
    pref_cond = item.get("ìš°ëŒ€ì¡°ê±´", "")
    join_target = item.get("ê°€ì…ëŒ€ìƒ", "")
    caution = item.get("ê¸°íƒ€ ìœ ì˜ì‚¬í•­", "")
    maturity_rate = item.get("ë§Œê¸°í›„ ì´ììœ¨", "")

    content = (
        f"ì€í–‰: {bank}\n"
        f"ìƒí’ˆëª…: {product}\n"
        f"ê¸°ë³¸ê¸ˆë¦¬( %): {base_rate}\n"
        f"ìµœê³ ê¸ˆë¦¬(ìš°ëŒ€ê¸ˆë¦¬í¬í•¨,  %): {max_rate}\n"
        f"ì´ìì§€ê¸‰ë°©ì‹: {interest_type}\n"
        f"ì€í–‰ ìµœì¢…ì œê³µì¼: {pub_date}\n"
        f"ìš°ëŒ€ì¡°ê±´: {pref_cond}\n"
        f"ê°€ì…ë°©ë²•: {join_method}\n"
        f"ê°€ì…ëŒ€ìƒ: {join_target}\n"
        f"ê¸°íƒ€ ìœ ì˜ì‚¬í•­: {caution}\n"
        f"ë§Œê¸°í›„ ì´ììœ¨: {maturity_rate}"
    )

    key_summary = (
        f"ì€í–‰: {bank}, ìƒí’ˆëª…: {product}, "
        f"ê¸°ë³¸ê¸ˆë¦¬( %): {base_rate}, ìµœê³ ê¸ˆë¦¬(ìš°ëŒ€ê¸ˆë¦¬í¬í•¨,  %): {max_rate}"
    )

    item["key_summary"] = key_summary

    doc = {
        "id": f"ì ê¸ˆ_{idx+1:03d}",
        "bank": bank,
        "product_name": product,
        "type": "ì ê¸ˆ",
        "content": content,
        "key_summary": key_summary,
        "metadata": item
    }
    documents.append(doc)


with open("installment_savings.json", "w", encoding="utf-8") as f:
    json.dump({"documents": documents}, f, ensure_ascii=False, indent=2)

print("ğŸ“ ì €ì¥ ì™„ë£Œ: installment_savings.json")
